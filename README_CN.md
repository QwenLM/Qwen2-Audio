<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png" width="400"/>
<p>

<p align="center">
Qwen2-Audio-7B <a href="https://modelscope.cn/models/qwen/Qwen2-Audio-7B">ğŸ¤– </a> | <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-Audio-7B-Instruct <a href="https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct">ğŸ¤– </a>| <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct">ğŸ¤—</a>&nbsp ï½œ Demo<a href="https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo"> ğŸ¤–</a> | <a href="https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo">ğŸ¤—</a>&nbsp
<br>
ğŸ“‘ <a href="https://arxiv.org/abs/2407.10759">Paper</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen2-audio">Blog</a> &nbsp&nbsp | &nbsp&nbsp ğŸ’¬ <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp
</p>



æˆ‘ä»¬ä»‹ç»Qwen-Audioçš„æœ€æ–°è¿›å±•ï¼šQwen2-Audioã€‚ä½œä¸ºä¸€ä¸ªå¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ŒQwen2-Audioèƒ½å¤Ÿæ¥å—å„ç§éŸ³é¢‘ä¿¡å·è¾“å…¥ï¼Œå¹¶æ ¹æ®è¯­éŸ³æŒ‡ä»¤æ‰§è¡ŒéŸ³é¢‘åˆ†ææˆ–ç›´æ¥å“åº”æ–‡æœ¬ã€‚æˆ‘ä»¬ä»‹ç»ä¸¤ç§ä¸åŒçš„éŸ³é¢‘äº¤äº’æ¨¡å¼ï¼šè¯­éŸ³èŠå¤© voice chat å’ŒéŸ³é¢‘åˆ†æ audio analysisã€‚

* è¯­éŸ³èŠå¤©ï¼šç”¨æˆ·å¯ä»¥è‡ªç”±åœ°ä¸ Qwen2-Audio è¿›è¡Œè¯­éŸ³äº’åŠ¨ï¼Œè€Œæ— éœ€æ–‡æœ¬è¾“å…¥ï¼›
* éŸ³é¢‘åˆ†æï¼šç”¨æˆ·å¯ä»¥åœ¨äº’åŠ¨è¿‡ç¨‹ä¸­æä¾›éŸ³é¢‘å’Œæ–‡æœ¬æŒ‡ä»¤å¯¹éŸ³é¢‘è¿›è¡Œåˆ†æï¼›

**æˆ‘ä»¬å·²ç»å¼€æºäº† Qwen2-Audio ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼šQwen2-Audio-7Bå’ŒQwen2-Audio-7B-Instructã€‚**

## æ¨¡å‹ç»“æ„ä¸è®­ç»ƒèŒƒå¼

Qwen2-Audio ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹æ¦‚è¿°ã€‚

<p align="left">
    <img src="assets/framework.png" width="80%"/>
<p>



## æ–°é—»
* 2024.8.9 ğŸ‰ æˆ‘ä»¬åœ¨ ModelScope å’Œ Hugging Face å¼€æºäº†`Qwen2-Audio-7B`å’Œ`Qwen2-Audio-7B-Instruct`çš„ checkpoint.
* 2024.7.15 ğŸ‰ æˆ‘ä»¬å‘å¸ƒäº† Qwen2-Audio çš„[è®ºæ–‡](https://arxiv.org/abs/2407.10759), ä»‹ç»äº†ç›¸å…³çš„æ¨¡å‹ç»“æ„ï¼Œè®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹è¡¨ç°ã€‚
* 2023.11.30 ğŸ”¥ æˆ‘ä»¬å‘å¸ƒäº†**Qwen-Audio**ç³»åˆ—
<br>

## è¯„æµ‹
æˆ‘ä»¬åœ¨æ ‡å‡†çš„13ä¸ªå­¦æœ¯æ•°æ®é›†ä¸Šè¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›å¦‚ä¸‹ï¼š
<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan="4">ASR</td><td rowspan="4">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan="4">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan="4"><a href="https://github.com/OFA-Sys/AIR-Bench">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>
<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>


ä»¥ä¸‹æ˜¯æ•´ä½“è¡¨ç°ï¼š
<p align="left">
    <img src="assets/radar_compare_qwen_audio.png" width="70%"/>
<p>

è¯„æµ‹åˆ†æ•°è¯¦æƒ…å¦‚ä¸‹ï¼š
<br>
<b>ï¼ˆæ³¨æ„ï¼šæˆ‘ä»¬æ‰€å±•ç¤ºçš„è¯„æµ‹ç»“æœæ˜¯åœ¨åŸå§‹è®­ç»ƒæ¡†æ¶çš„åˆå§‹æ¨¡å‹ä¸Šçš„ï¼Œç„¶è€Œåœ¨æ¡†æ¶è½¬æ¢ Huggingface åæŒ‡æ ‡å‡ºç°äº†éƒ¨åˆ†æ³¢åŠ¨ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„å…¨éƒ¨æµ‹è¯„ç»“æœï¼šé¦–å…ˆæ˜¯è®ºæ–‡ä¸­çš„åˆå§‹æ¨¡å‹ç»“æœï¼‰</b>

<table><thead><tr><th rowspan="2">Task</th><th rowspan="2">Dataset</th><th rowspan="2">Model</th><th colspan="2">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan="15">ASR</td><td rowspan="7"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan="7">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan="2"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan="2">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>
<tr><td rowspan="2"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan="2">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan="4"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan="4">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan="8">S2TT</td><td rowspan="5"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan="5">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>
<tr><td rowspan="3"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan="3">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan="3">SER</td><td rowspan="3"><b>Meld</b></td><td>WavLM-large</td><td rowspan="3">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan="4">VSC</td><td rowspan="4"><b>VocalSound</b></td><td>CLAP</td><td rowspan="4">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>
<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>

<b>ï¼ˆå…¶æ¬¡æ˜¯è½¬æ¢ huggingface åçš„ï¼‰</b>

<table><thead><tr><th rowspan="2">Task</th><th rowspan="2">Dataset</th><th rowspan="2">Model</th><th colspan="2">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan="15">ASR</td><td rowspan="7"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan="7">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan="2"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan="2">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>
<tr><td rowspan="2"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan="2">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan="4"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan="4">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan="8">S2TT</td><td rowspan="5"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan="5">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>
<tr><td rowspan="3"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan="3">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan="3">SER</td><td rowspan="3"><b>Meld</b></td><td>WavLM-large</td><td rowspan="3">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan="4">VSC</td><td rowspan="4"><b>VocalSound</b></td><td>CLAP</td><td rowspan="4">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>
<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>


æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## éƒ¨ç½²è¦æ±‚
The code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:
Qwen2-Audioçš„ä»£ç å·²ç»åŒ…å«åœ¨æœ€æ–°çš„ Hugging Face Transformers çš„ä¸»åˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨å‘½ä»¤`pip install git+https://github.com/huggingface/transformers`
```
KeyError: 'qwen2-audio'
```
## å¿«é€Ÿä½¿ç”¨
æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Qwen2-Audio-7B å’Œ Qwen2-Audio-7B-Instructã€‚
åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚
æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨ Transformers æˆ–è€… ModelScope æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç›®å‰Qwen2-Audio-7B åŠ Qwen2-Audio-7B-Instruct æ¨¡å‹å¤„ç†30ç§’ä»¥å†…çš„éŸ³é¢‘è¡¨ç°æ›´ä½³ã€‚
#### ğŸ¤— Hugging Face Transformers
å¦‚å¸Œæœ›ä½¿ç”¨ Qwen2-Audio-7B-Instruct è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬åˆ†åˆ«æ¼”ç¤ºè¯­éŸ³èŠå¤©å’ŒéŸ³é¢‘åˆ†æçš„äº¤äº’æ–¹å¼ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚
##### è¯­éŸ³èŠå¤©æ¨ç†
åœ¨è¯­éŸ³èŠå¤©æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±åœ°ä¸ Qwen2-Audio è¿›è¡Œè¯­éŸ³äº¤äº’ï¼Œæ— éœ€æ–‡å­—è¾“å…¥ï¼š
```python
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")

conversation = [
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav"},
    ]},
    {"role": "assistant", "content": "Yes, the speaker is female and in her twenties."},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav"},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios = []
for message in conversation:
    if isinstance(message["content"], list):
        for ele in message["content"]:
            if ele["type"] == "audio":
                audios.append(librosa.load(
                    BytesIO(urlopen(ele['audio_url']).read()), 
                    sr=processor.feature_extractor.sampling_rate)[0]
                )

inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
inputs.input_ids = inputs.input_ids.to("cuda")

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```
##### éŸ³é¢‘åˆ†ææ¨ç†
åœ¨éŸ³é¢‘åˆ†æä¸­ï¼Œç”¨æˆ·å¯ä»¥æä¾›éŸ³é¢‘å’Œæ–‡å­—é—®é¢˜æ¥å®ç°å¯¹éŸ³é¢‘çš„åˆ†æï¼š
```python
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")

conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'}, 
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"},
        {"type": "text", "text": "What's that sound?"},
    ]},
    {"role": "assistant", "content": "It is the sound of glass shattering."},
    {"role": "user", "content": [
        {"type": "text", "text": "What can you do when you hear that?"},
    ]},
    {"role": "assistant", "content": "Stay alert and cautious, and check if anyone is hurt or if there is any damage to property."},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac"},
        {"type": "text", "text": "What does the person say?"},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios = []
for message in conversation:
    if isinstance(message["content"], list):
        for ele in message["content"]:
            if ele["type"] == "audio":
                audios.append(
                    librosa.load(
                        BytesIO(urlopen(ele['audio_url']).read()), 
                        sr=processor.feature_extractor.sampling_rate)[0]
                )

inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
inputs.input_ids = inputs.input_ids.to("cuda")

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```
##### æ‰¹é‡æ¨ç†
æˆ‘ä»¬ä¹Ÿæ”¯æŒæ‰¹é‡æ¨ç†ï¼š
```python
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")

conversation1 = [
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"},
        {"type": "text", "text": "What's that sound?"},
    ]},
    {"role": "assistant", "content": "It is the sound of glass shattering."},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav"},
        {"type": "text", "text": "What can you hear?"},
    ]}
]

conversation2 = [
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac"},
        {"type": "text", "text": "What does the person say?"},
    ]},
]

conversations = [conversation1, conversation2]

text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]

audios = []
for conversation in conversations:
    for message in conversation:
        if isinstance(message["content"], list):
            for ele in message["content"]:
                if ele["type"] == "audio":
                    audios.append(
                        librosa.load(
                            BytesIO(urlopen(ele['audio_url']).read()), 
                            sr=processor.feature_extractor.sampling_rate)[0]
                    )

inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
inputs['input_ids'] = inputs['input_ids'].to("cuda")
inputs.input_ids = inputs.input_ids.to("cuda")

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```
è¿è¡ŒQwen2-Audio-7BåŒæ ·éå¸¸ç®€å•ã€‚
```python
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration

model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B" ,trust_remote_code=True)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B" ,trust_remote_code=True)

prompt = "<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:"
url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3"
audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)
inputs = processor(text=prompt, audios=audio, return_tensors="pt")

generated_ids = model.generate(**inputs, max_length=256)
generated_ids = generated_ids[:, inputs.input_ids.size(1):]
response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```

#### Finetuning
æ„Ÿè°¢ Hugging Face å¼€æºç¤¾åŒºçš„è´¡çŒ®ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åœ°ä½¿ç”¨ Accelerate å’Œ DeepSpeed å®ç°æ¨¡å‹å¾®è°ƒï¼ˆfinetuningï¼‰ã€‚è„šæœ¬æ”¯æŒ LoRAï¼ˆä½ç§©é€‚åº”ï¼‰å’Œå…¨å‚æ•°å¾®è°ƒï¼Œç›¸å…³ä»£ç æ¥è‡ª[Xiaoming Liu](https://github.com/Lollipop)ã€‚

```bash
cd finetune && bash run.sh
```

#### ğŸ¤– ModelScope
æˆ‘ä»¬å¼ºçƒˆå»ºè®®ç”¨æˆ·ï¼Œç‰¹åˆ«æ˜¯ä¸­å›½å¤§é™†åœ°åŒºçš„ç”¨æˆ·ï¼Œä½¿ç”¨ ModelScopeã€‚`snapshot_download` å¯ä»¥å¸®åŠ©æ‚¨è§£å†³ä¸‹è½½æ£€æŸ¥ç‚¹æ—¶é‡åˆ°çš„é—®é¢˜ã€‚
<br>
## Demo
### Web UI
æˆ‘ä»¬æä¾›äº† Web UI çš„ demo ä¾›ç”¨æˆ·ä½¿ç”¨ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š
```
pip install -r requirements_web_demo.txt
```
éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š
```
python demo/web_demo_audio.py
```
<br>

## æ ·ä¾‹å±•ç¤º
æ›´å¤šæ ·ä¾‹å°†æ›´æ–°äº[é€šä¹‰åƒé—®åšå®¢](https://qwenlm.github.io/blog/qwen2-audio)ä¸­çš„ Qwen2-Audio åšå®¢ã€‚

## å›¢é˜Ÿæ‹›è˜

æˆ‘ä»¬æ˜¯é€šä¹‰åƒé—®è¯­éŸ³å¤šæ¨¡æ€å›¢é˜Ÿï¼Œè‡´åŠ›äºä»¥é€šä¹‰åƒé—®ä¸ºæ ¸å¿ƒï¼Œæ‹“å±•éŸ³é¢‘å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°è‡ªç”±çµæ´»çš„éŸ³é¢‘äº¤äº’ã€‚ç›®å‰å›¢é˜Ÿè“¬å‹ƒå‘å±•ä¸­ï¼Œå¦‚æœ‰æ„å‘å®ä¹ æˆ–å…¨èŒåŠ å…¥æˆ‘ä»¬ï¼Œè¯·å‘é€ç®€å†è‡³ `qwen_audio@list.alibaba-inc.com`.
<br>

## ä½¿ç”¨åè®®

è¯·æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹åœ¨å…¶ Hugging Face ä»“åº“ä¸­çš„è®¸å¯è¯ã€‚æ‚¨æ— éœ€æäº¤å•†ä¸šä½¿ç”¨ç”³è¯·ã€‚
<br>



## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ :star: å’Œå¼•ç”¨ :pencil: :)

```BibTeX
@article{Qwen-Audio,
  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}
```

```BibTeX
@article{Qwen2-Audio,
  title={Qwen2-Audio Technical Report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}
```
## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆ`qianwen_opensource@alibabacloud.com`ï¼‰è”ç³»æˆ‘ä»¬ã€‚
<br>
